/**
 *
 * Carlos III University of Madrid.
 *
 * Master Final Thesis: Heartbeat classifier based on ANN (Artificial Neural
 * Network).
 *
 * Author: Pedro Marcos Solórzano
 * Tutor: Luis Mengibar Pozo (Tutor)
 *
 *
 * Back-propagation training for feedforward ANN
 * Source file
 *
 *
 */

#include "Training.h"

/*
 * Empty constructor
 */
Training::Training (){}


/*
 * Constructor method.
 *
 * It creates a new ANN and trains it. These parameters
 * must be set:
 * - number of layers, including input & output layers. (numLayer)
 * - number of neurons in each layer (layerSize)
 * - momentum for the training (momentum)
 * - learning rate (learnRate)
 * - delta errors for each neuron (deltaErr)
 */
Training::Training(int numLayer, int *layerSize, double momentum,
                   double learnRate, double **deltaErr){
  int i, j, k;
  double ***weight;
  /*
   * Memory allocation.
   * Take into account the first layer's neurons (input) don't have neither
   * weights nor delta error
   */
  _deltaErr = new double*[numLayer];
  for(i=1; i<numLayer; ++i)
    _deltaErr[i] = new double[layerSize[i]];

  weight = new double**[numLayer];
  for(i=1; i<numLayer; ++i)
    weight[i]=new double*[layerSize[i]];
  for(i=1; i<numLayer; ++i)
    for(j=0; j<layerSize[i]; ++j)
      weight[i][j] = new double[layerSize[i-1]+1];

  _prevWeight = new double**[numLayer];
  for(i=1; i<numLayer; ++i)
    _prevWeight[i]=new double*[layerSize[i]];
  for(i=1; i<numLayer; ++i)
    for(j=0; j<layerSize[i]; ++j)
      _prevWeight[i][j] = new double[layerSize[i-1]+1];

  /*
   * Save random weights in the matrix
   */
  for(i=1; i<numLayer; ++i)
    for(j=0; j<layerSize[i]; ++j)
      for(k=0; k<layerSize[i-1]+1; ++k)
	weight[i][j][k] = (double)(rand())/(RAND_MAX/2) - 1;

  /*
   * Previous weights initialization
   */
  for(i=1; i<numLayer; ++i)
    for(j=0; j<layerSize[i]; ++j)
      for(k=0; k<layerSize[i-1]+1; ++k)
	_prevWeight[i][j][k] = 0;

  /*
   * Data copy and untrained ANN creation.
   */
  _learnRate = learnRate;

  _momentum = momentum;

  _ann = new ANN(numLayer, layerSize, weight);
}

/*
 * Destructor
 */
Training::~Training ()
{
  int i,j;
  /*
   * Free all dynamic memory
   */
  for(i=1; i<_ann->getNumLayer(); ++i)
    for(j=0; j<_ann->getLayerSize(i); ++j)
      delete[] _prevWeight[i][j];
  for(i=1; i<_ann->getNumLayer(); ++i)
    delete[] _prevWeight[i];
  delete[] _prevWeight;

  for(i=1; i<_ann->getNumLayer(); ++i)
    delete[] _deltaErr[i];
  delete[] _deltaErr;

  delete _ann;
}

/*
 * Training method.
 *
 * The training is performed introducing a Matrix (trainMat) with possible
 * inputs beside their expected outputs. The matrix format must be:
 * ( In  , In  , ... , ... , Out , Out )
 * ( In  , In  , ... , ... , Out , Out )
 * ( ... , ... , ... , ... , ... , ... )
 *
 * where numInputs is the number of rows.
 *
 * The training goal is get the desired ANN Squared Error defined in
 * minSquareErr, but it must be limited to a maximum number of interactions
 * (maxInter)
 *
 * Check Back-Propagation training's documentation for more information
 * about the performance
 */
double Training::backpropagation(double **trainMat, int numInputs, int maxInter,
                                 int minSquareErr){

}

